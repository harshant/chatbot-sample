{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq-to-seq sample implementation.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FojhEtpC8bNu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np , os\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bs3MemPX8gDV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "66933cb1-784b-4fb2-cac9-819b5feaf625"
      },
      "cell_type": "code",
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'dataset.zip', origin='http://ec2-18-232-83-49.compute-1.amazonaws.com/tst2012.zip', \n",
        "    extract=True)\n",
        "\n",
        "path_from = os.path.dirname(path_to_zip)+\"/tst2012.from\"\n",
        "path_to = os.path.dirname(path_to_zip)+\"/tst2012.to\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ec2-18-232-83-49.compute-1.amazonaws.com/tst2012.zip\n",
            "\r8192/7845 [===============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C6e27T148k9U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_dataset(words, n_words):\n",
        "    count = [['GO', 0], ['PAD', 1], ['EOS', 2], ['UNK', 3]]\n",
        "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
        "    dictionary = dict()\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        index = dictionary.get(word, 0)\n",
        "        if index == 0:\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s3-PfTO_8o95",
        "colab_type": "code",
        "outputId": "4b6854c0-1efe-471d-e24a-d43f63391c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text_from = io.open(path_from, encoding='UTF-8').read().strip().split('\\n')\n",
        "text_to = io.open(path_to, encoding='UTF-8').read().strip().split('\\n')\n",
        "print('len from: %d, len to: %d'%(len(text_from), len(text_to)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len from: 100, len to: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GjFUMmXo8trW",
        "colab_type": "code",
        "outputId": "0fe0601d-e774-45ea-e086-d678ead2afda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "concat_from = ' '.join(text_from).split()\n",
        "vocabulary_size_from = len(list(set(concat_from)))\n",
        "data_from, count_from, dictionary_from, rev_dictionary_from = build_dataset(concat_from, vocabulary_size_from)\n",
        "print('vocab from size: %d'%(vocabulary_size_from))\n",
        "print('Most common words', count_from[4:10])\n",
        "print('Sample data', data_from[:10], [rev_dictionary_from[i] for i in data_from[:10]])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab from size: 722\n",
            "Most common words [('.', 89), (\"'\", 53), ('I', 46), (',', 46), ('the', 44), ('to', 41)]\n",
            "Sample data [126, 5, 24, 25, 217, 26, 20, 127, 218, 47] ['Aren', \"'\", 't', 'they', 'streaming', 'it', 'for', 'free', 'online', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kcVqAQjr80zN",
        "colab_type": "code",
        "outputId": "777bbfdc-ff6d-45a5-8cac-0471c21ddab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "concat_to = ' '.join(text_to).split()\n",
        "vocabulary_size_to = len(list(set(concat_to)))\n",
        "data_to, count_to, dictionary_to, rev_dictionary_to = build_dataset(concat_to, vocabulary_size_to)\n",
        "print('vocab to size: %d'%(vocabulary_size_to))\n",
        "print('Most common words', count_to[4:10])\n",
        "print('Sample data', data_to[:10], [rev_dictionary_to[i] for i in data_to[:10]])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab to size: 645\n",
            "Most common words [('.', 89), (\"'\", 48), ('I', 46), ('^', 40), ('the', 38), (',', 37)]\n",
            "Sample data [106, 9, 171, 28, 50, 4, 107, 108, 172, 4] ['Yes', ',', 'yes', 'they', 'are', '.', 'That', 'poor', 'bastard', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T4IKibJz84Tn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GO = dictionary_from['GO']\n",
        "PAD = dictionary_from['PAD']\n",
        "EOS = dictionary_from['EOS']\n",
        "UNK = dictionary_from['UNK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTwaxqo0882W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Defining seq2seq model\n",
        "class Chatbot:\n",
        "    def __init__(self, size_layer, num_layers, embedded_size,\n",
        "                 from_dict_size, to_dict_size, learning_rate, batch_size):\n",
        "        \n",
        "        def cells(reuse=False):\n",
        "            return tf.nn.rnn_cell.LSTMCell(size_layer,initializer=tf.orthogonal_initializer(),reuse=reuse)\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, None])\n",
        "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
        "        self.X_seq_len = tf.placeholder(tf.int32, [None])\n",
        "        self.Y_seq_len = tf.placeholder(tf.int32, [None])\n",
        "\n",
        "        with tf.variable_scope(\"encoder_embeddings\"):        \n",
        "            \n",
        "            encoder_embeddings = tf.Variable(tf.random_uniform([from_dict_size, embedded_size], -1, 1))\n",
        "            print(encoder_embeddings)\n",
        "            encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
        "            print(encoder_embedded)\n",
        "            main = tf.strided_slice(self.X, [0, 0], [batch_size, -1], [1, 1])\n",
        "            \n",
        "        with tf.variable_scope(\"decoder_embeddings\"):        \n",
        "            decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
        "            decoder_embeddings = tf.Variable(tf.random_uniform([to_dict_size, embedded_size], -1, 1))\n",
        "            decoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, decoder_input)\n",
        "        \n",
        "        with tf.variable_scope(\"encoder\"):\n",
        "            rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
        "            _, last_state = tf.nn.dynamic_rnn(rnn_cells, encoder_embedded,\n",
        "                                              dtype = tf.float32)\n",
        "        with tf.variable_scope(\"decoder\"):\n",
        "            rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell([cells() for _ in range(num_layers)])\n",
        "            outputs, _ = tf.nn.dynamic_rnn(rnn_cells_dec, decoder_embedded, \n",
        "                                           initial_state = last_state,\n",
        "                                           dtype = tf.float32)\n",
        "        with tf.variable_scope(\"logits\"):            \n",
        "            self.logits = tf.layers.dense(outputs,to_dict_size)\n",
        "            print(self.logits)\n",
        "            masks = tf.sequence_mask(self.Y_seq_len, tf.reduce_max(self.Y_seq_len), dtype=tf.float32)\n",
        "        with tf.variable_scope(\"cost\"):            \n",
        "            self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.logits,\n",
        "                                                         targets = self.Y,\n",
        "                                                         weights = masks)\n",
        "        with tf.variable_scope(\"optimizer\"):            \n",
        "            self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZxZkDvV59ecI",
        "colab_type": "code",
        "outputId": "89d7c487-30d6-457c-99b9-2cde58ab252c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "cell_type": "code",
      "source": [
        "#Hyperparameters         \n",
        "            \n",
        "size_layer = 128\n",
        "num_layers = 2\n",
        "embedded_size = 128\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epoch = 1\n",
        "\n",
        "#Training\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "model = Chatbot(size_layer, num_layers, embedded_size, vocabulary_size_from + 4, \n",
        "                vocabulary_size_to + 4, learning_rate, batch_size)\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "saver = tf.train.Saver(tf.global_variables(), max_to_keep=2)\n",
        "checkpoint_dir = os.path.abspath(os.path.join('./', \"checkpoints_chatbot\"))\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "<tf.Variable 'encoder_embeddings/Variable:0' shape=(726, 128) dtype=float32_ref>\n",
            "Tensor(\"encoder_embeddings/embedding_lookup/Identity:0\", shape=(?, ?, 128), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-8-fa9d5f697663>:6: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-8-fa9d5f697663>:27: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-8-fa9d5f697663>:29: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From <ipython-input-8-fa9d5f697663>:36: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "Tensor(\"logits/dense/BiasAdd:0\", shape=(32, ?, 649), dtype=float32)\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P-r1qTUd9nRy",
        "colab_type": "code",
        "outputId": "9d21eead-078a-4605-de97-ed92a18c73ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 -c 'import tensorflow as tf; print(tf.__version__)'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iiHg5uHK9qID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def str_idx(corpus, dic):\n",
        "    X = []\n",
        "    for i in corpus:\n",
        "        ints = []\n",
        "        for k in i.split():\n",
        "            try:\n",
        "                ints.append(dic[k])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                ints.append(2)\n",
        "        X.append(ints)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NwR71hJM9tl7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    padded_seqs = []\n",
        "    seq_lens = []\n",
        "    max_sentence_len = 50\n",
        "    for sentence in sentence_batch:\n",
        "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
        "        seq_lens.append(50)\n",
        "    return padded_seqs, seq_lens\n",
        "\n",
        "def check_accuracy(logits, Y):\n",
        "    acc = 0\n",
        "    for i in range(logits.shape[0]):\n",
        "        internal_acc = 0\n",
        "        for k in range(len(Y[i])):\n",
        "            if Y[i][k] == logits[i][k]:\n",
        "                internal_acc += 1\n",
        "        acc += (internal_acc / len(Y[i]))\n",
        "    return acc / logits.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MMIpcWB49yL-",
        "colab_type": "code",
        "outputId": "b9272922-aaaf-447d-9a92-f2c3e494177b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X = str_idx(text_from, dictionary_from)\n",
        "Y = str_idx(text_to, dictionary_to)\n",
        "\n",
        "\n",
        "for i in range(epoch):\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    print(len(text_from))\n",
        "    for k in range(0, (len(text_from) // batch_size) * batch_size, batch_size):\n",
        "        batch_x, seq_x = pad_sentence_batch(X[k: k+batch_size], PAD)\n",
        "        batch_y, seq_y = pad_sentence_batch(Y[k: k+batch_size], PAD)\n",
        "        #print(batch_x)\n",
        "        #print(type(batch_x))\n",
        "        #print(np.asarray(batch_x).reshape(32,1))\n",
        "        arr_x = np.zeros([32,50])\n",
        "        arr_y = np.zeros([32,50])\n",
        "        arr_seq_x = np.zeros([32])\n",
        "        arr_seq_y = np.zeros([32])\n",
        "        \n",
        "        # converting list to array to pass through feed_dict \n",
        "        for s in range(32):\n",
        "          for k in range(50):\n",
        "            arr_x[s][k]=batch_x[s][k]\n",
        "        for s in range(32):\n",
        "          for k in range(50):\n",
        "            arr_y[s][k]=batch_y[s][k]\n",
        "        for s in range(32):\n",
        "          arr_seq_x[s]=seq_x[s]\n",
        "        for s in range(32):\n",
        "          arr_seq_y[s]=seq_y[s]          \n",
        "          \n",
        "        predicted, loss, _ = sess.run([tf.argmax(model.logits,2), model.cost, model.optimizer], \n",
        "                                      feed_dict={model.X:arr_x,\n",
        "                                                model.Y:arr_y,\n",
        "                                                model.X_seq_len:arr_seq_x,\n",
        "                                                model.Y_seq_len:arr_seq_y})\n",
        "        \n",
        "        total_loss += loss\n",
        "#        total_accuracy += check_accuracy(predicted,batch_y)\n",
        "#        print 'output:', [rev_dictionary_to[i] for i in predicted[0]]\n",
        "#        print 'input:', [rev_dictionary_to[i] for i in batch_x[0]]\n",
        "        \n",
        "    total_loss /= (len(text_from) // batch_size)\n",
        "    total_accuracy /= (len(text_from) // batch_size)\n",
        "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, total_loss, total_accuracy))\n",
        "    path = saver.save(sess, checkpoint_prefix, global_step=i+1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'O'\n",
            "'Knight'\n",
            "100\n",
            "epoch: 1, avg loss: 6.340613, avg accuracy: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lb4zAfX995qW",
        "colab_type": "code",
        "outputId": "fb91803b-f5ba-4bc0-8adb-4e7eab11d9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "#Evaluation\n",
        "\n",
        "def predict(sentence):\n",
        "    X_in = []\n",
        "    for word in sentence.split():\n",
        "        try:\n",
        "            X_in.append(dictionary_from[word])\n",
        "        except:\n",
        "            X_in.append(PAD)\n",
        "            pass\n",
        "        \n",
        "    test, seq_x = pad_sentence_batch([X_in], PAD)\n",
        "    input_batch = np.zeros([batch_size,seq_x[0]])\n",
        "    input_batch[0] =test[0] \n",
        "        \n",
        "    log = sess.run(tf.argmax(model.logits,2), \n",
        "                                      feed_dict={\n",
        "                                              model.X:input_batch,\n",
        "                                              model.X_seq_len:seq_x,\n",
        "                                              model.Y_seq_len:seq_x\n",
        "                                              }\n",
        "                                      )\n",
        "    \n",
        "    result=' '.join(rev_dictionary_to[i] for i in log[0])\n",
        "    return result\n",
        "    \n",
        "checkpoint_file = tf.train.latest_checkpoint(os.path.join('./', 'checkpoints_chatbot'))\n",
        "saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
        "saver.restore(sess, checkpoint_file)\n",
        "    \n",
        "print (predict('how are you ?')) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/checkpoints_chatbot/model-1\n",
            "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}